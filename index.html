<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Terrence Neumann </title> <meta name="author" content="Terrence Neumann"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://terryneumann.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Terrence</span> Neumann </h1> <p class="desc">PhD Student at the <a href="https://www.mccombs.utexas.edu/faculty-research/departments/irom/" rel="external nofollow noopener" target="_blank">University of Texas at Austin</a>.</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/headshot-480.webp 480w,/assets/img/headshot-800.webp 800w,/assets/img/headshot-1400.webp 1400w," type="image/webp" sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/headshot.jpg?f04cedd83ca74fb5c8edc18efca88c52" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="headshot.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <h2 id="about-me">About me</h2> <p>I am a fifth-year PhD candidate at the University of Texas at Austin studying Information Systems, advised by <a href="https://mariadearteaga.com/" rel="external nofollow noopener" target="_blank">Maria De-Arteaga</a> and <a href="https://yleng.github.io/www/" rel="external nofollow noopener" target="_blank">Yan Leng</a>. Throughout my PhD, I’ve been fortunate to collaborate with and learn from great researchers like <a href="https://cssh.northeastern.edu/faculty/sina-fazelpour/" rel="external nofollow noopener" target="_blank">Sina Fazelpour</a>, <a href="https://mattlease.com/" rel="external nofollow noopener" target="_blank">Matt Lease</a>, and <a href="https://www.maytals.com/" rel="external nofollow noopener" target="_blank">Maytal Saar-Tsechansky</a>.</p> <p>My research focuses on <strong>trustworthy AI</strong>. I have found the <a href="https://www.nist.gov/itl/ai-risk-management-framework" rel="external nofollow noopener" target="_blank">NIST AI Risk Management Framework’s Characteristics of Trustworthy AI</a> as a valuable framework for scoping and defining my contributions. I am particularly interested in the following research streams: (1) mechanistic interpretability of LLM agents, (2) responsible use of LLMs as silicon subjects for academic and social applications, and (3) algorithmic fairness on social media. I pursue research that bridges machine learning and computational social science to address pressing challenges at the intersection of AI and society.</p> <div class="trustworthy-ai-framework"> <div class="framework-container"> <svg viewbox="0 0 1200 300" xmlns="http://www.w3.org/2000/svg" class="framework-svg"> <g class="framework-box" data-category="safe" role="button" tabindex="0"> <rect x="20" y="20" width="140" height="100" rx="5" class="box-rect"></rect> <text x="90" y="75" text-anchor="middle" class="box-text">Safe</text> </g> <g class="framework-box" data-category="secure" role="button" tabindex="0"> <rect x="180" y="20" width="180" height="100" rx="5" class="box-rect"></rect> <text x="270" y="60" text-anchor="middle" class="box-text">Secure &amp;</text> <text x="270" y="85" text-anchor="middle" class="box-text">Resilient</text> </g> <g class="framework-box" data-category="explainable" role="button" tabindex="0"> <rect x="380" y="20" width="200" height="100" rx="5" class="box-rect"></rect> <text x="480" y="60" text-anchor="middle" class="box-text">Explainable &amp;</text> <text x="480" y="85" text-anchor="middle" class="box-text">Interpretable</text> </g> <g class="framework-box" data-category="privacy" role="button" tabindex="0"> <rect x="600" y="20" width="180" height="100" rx="5" class="box-rect"></rect> <text x="690" y="60" text-anchor="middle" class="box-text">Privacy-</text> <text x="690" y="85" text-anchor="middle" class="box-text">Enhanced</text> </g> <g class="framework-box" data-category="fair" role="button" tabindex="0"> <rect x="800" y="20" width="200" height="100" rx="5" class="box-rect"></rect> <text x="900" y="55" text-anchor="middle" class="box-text">Fair - With Harmful</text> <text x="900" y="80" text-anchor="middle" class="box-text">Bias Managed</text> </g> <g class="framework-box" data-category="accountable" role="button" tabindex="0"> <rect x="1020" y="20" width="160" height="100" rx="5" class="box-rect"></rect> <text x="1100" y="55" text-anchor="middle" class="box-text">Accountable &amp;</text> <text x="1100" y="80" text-anchor="middle" class="box-text">Transparent</text> </g> <g class="framework-box framework-box-bottom" data-category="valid" role="button" tabindex="0"> <rect x="20" y="180" width="1160" height="100" rx="5" class="box-rect"></rect> <text x="600" y="237" text-anchor="middle" class="box-text box-text-large">Valid &amp; Reliable</text> </g> <g class="framework-box framework-box-all active" data-category="all" role="button" tabindex="0"> <rect x="500" y="145" width="200" height="25" rx="5" class="box-rect box-all-rect"></rect> <text x="600" y="162" text-anchor="middle" class="box-text box-all-text">Show All</text> </g> </svg> </div> </div> <style>.trustworthy-ai-framework{margin:2rem 0;width:100%}.framework-container{max-width:100%;margin:0 auto}.framework-svg{width:100%;height:auto}.framework-box{cursor:pointer;transition:all .3s ease}.box-rect{fill:var(--global-text-color-light,#6c7a89);stroke:var(--global-text-color,#111);stroke-width:2;transition:all .3s ease}.box-text{fill:var(--global-bg-color,#fff);font-size:20px;font-weight:500;pointer-events:none;font-family:var(--global-font-family)}.box-text-large{font-size:26px;font-weight:600}.box-all-rect{fill:var(--global-theme-color,#4682b4);stroke-width:1.5}.box-all-text{font-size:14px;font-weight:600}.framework-box:hover .box-rect{fill:var(--global-theme-color,#4682b4);transform:scale(1.02)}.framework-box.active .box-rect{fill:var(--global-theme-color,#4682b4);stroke-width:3;filter:brightness(1.1)}.framework-box:focus{outline:2px solid var(--global-theme-color,#4682b4);outline-offset:2px}html[data-theme="dark"] .box-rect{fill:var(--global-theme-color,#4682b4);stroke:var(--global-theme-color,#4682b4)}html[data-theme="dark"] .box-text{fill:var(--global-text-color,#ddd)}html[data-theme="dark"] .box-all-rect{fill:var(--global-theme-color,#4682b4)}html[data-theme="dark"] .framework-box:hover .box-rect{fill:#5a9bd4;filter:brightness(1.15)}html[data-theme="dark"] .framework-box.active .box-rect{fill:#8b5fbf;stroke:#8b5fbf;filter:brightness(1)}@media(max-width:768px){.box-text{font-size:12px}.box-text-large{font-size:16px}.box-all-text{font-size:11px}}</style> <script>
(function() {
  // Wait for everything to be fully loaded
  window.addEventListener('load', function() {
    setTimeout(initFramework, 100); // Small delay to ensure bibliography is rendered
  });

  function initFramework() {
    const frameworkBoxes = document.querySelectorAll('.framework-box');
    const publications = document.querySelectorAll('.publications ol.bibliography > li');

    console.log('Framework boxes found:', frameworkBoxes.length);
    console.log('Publications found:', publications.length);

    // Add click handlers to framework boxes
    frameworkBoxes.forEach(box => {
      box.addEventListener('click', function() {
        handleCategoryClick(this);
      });

      // Keyboard accessibility
      box.addEventListener('keypress', function(e) {
        if (e.key === 'Enter' || e.key === ' ') {
          e.preventDefault();
          handleCategoryClick(this);
        }
      });
    });

    function handleCategoryClick(box) {
      const category = box.getAttribute('data-category');
      console.log('Clicked category:', category);

      // Remove active class from all boxes
      frameworkBoxes.forEach(b => b.classList.remove('active'));

      // Add active class to clicked box
      box.classList.add('active');

      // Re-query publications to get fresh list
      const currentPublications = document.querySelectorAll('.publications ol.bibliography > li');
      console.log('Current publications:', currentPublications.length);

      // Filter publications
      if (category === 'all') {
        // Show all publications
        currentPublications.forEach(pub => {
          pub.style.display = '';
        });
      } else {
        // Filter by category
        let matchCount = 0;
        currentPublications.forEach(pub => {
          // Get the data attribute from the .row div inside the li
          const rowDiv = pub.querySelector('.row');
          const categories = rowDiv ? rowDiv.getAttribute('data-trustworthy-ai-categories') : null;
          console.log('Paper categories:', categories);
          if (categories && categories.split(',').includes(category)) {
            pub.style.display = '';
            matchCount++;
          } else {
            pub.style.display = 'none';
          }
        });
        console.log('Matched papers:', matchCount);
      }

      // Smooth scroll to publications
      const publicationsSection = document.querySelector('.publications');
      if (publicationsSection) {
        setTimeout(() => {
          publicationsSection.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
        }, 100);
      }
    }
  }
})();
</script> <p>Above: NIST’s <strong>Characteristics of Trustworthy AI</strong>. Click on a characteristic to explore my work in that area.</p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row" data-trustworthy-ai-categories="valid"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rq3_plot_strong_nonbinary-480.webp 480w,/assets/img/publication_preview/rq3_plot_strong_nonbinary-800.webp 800w,/assets/img/publication_preview/rq3_plot_strong_nonbinary-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/rq3_plot_strong_nonbinary.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rq3_plot_strong_nonbinary.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="neumann2025should" class="col-sm-8"> <div class="title">Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation</div> <div class="author"> Terrence Neumann, Maria De-Arteaga, and Sina Fazelpour </div> <div class="periodical"> <em>Forthcoming in Proceedings of the Fortieth AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2504.08954" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The emergent capabilities of large language models (LLMs) have prompted interest in using them as surrogates for human subjects in opinion surveys. However, prior evaluations of LLM-based opinion simulation have relied heavily on costly, domain-specific survey data, and mixed empirical results leave their reliability in question. To enable cost-effective, early-stage evaluation, we introduce a quality control assessment designed to test the viability of LLM-simulated opinions on Likert-scale tasks without requiring large-scale human data for validation. This assessment comprises two key tests: logical consistency and alignment with stakeholder expectations, offering a low-cost, domain-adaptable validation tool. We apply our quality control assessment to an opinion simulation task relevant to AI-assisted content moderation and fact-checking workflows—a socially impactful use case—and evaluate seven LLMs using a baseline prompt engineering method (backstory prompting), as well as fine-tuning and in-context learning variants. None of the models or methods pass the full assessment, revealing several failure modes. We conclude with a discussion of the risk management implications and release TopicMisinfo, a benchmark dataset with paired human and LLM annotations simulated by various models and approaches, to support future research.</p> </div> </div> </div> </li> <li> <div class="row" data-trustworthy-ai-categories="explainable"> <div class="col col-sm-2 abbr"> </div> <div id="neumann2025investigating" class="col-sm-8"> <div class="title">From Statistical Patterns Emerge Human-Like Behaviors: How LLMs Learn Social Preferences</div> <div class="author"> Terrence Neumann, and Yan Leng </div> <div class="periodical"> <em>Working Paper</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) are increasingly deployed in settings where their decisions carry ethical and interpersonal consequences. Despite being trained solely through next-token prediction, LLMs often display emergent social behaviors and preferences, such as altruism, fairness, and self-interest. Yet the mechanisms underlying these behaviors remain poorly understood, raising critical questions for both theory and deployment: Why do such behaviors emerge, and can they be controlled? This paper provides an initial answer by linking external social behavior to internal model computation. We introduce a two-step framework using Sparse Autoencoders (SAEs) to identify and manipulate latent features associated with human-like preferences. Using the dictator game—a canonical testbed from behavioral economics—we show that certain latent directions in an open-source LLM (Gemma 2 9b-IT) align with altruistic and self-interested concepts. Activating a single “self-interest” feature, for instance, nearly doubles the rate of self-serving decisions (from 35% to 72.5%). These results demonstrate that social preferences in LLMs can be traced and steered via an interpretable latent structure, providing a foundation for more transparent, controllable, and norm-sensitive AI behavior. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row" data-trustworthy-ai-categories="fair,valid"> <div class="col col-sm-2 abbr"> </div> <div id="neumann2024diverse" class="col-sm-8"> <div class="title">Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation</div> <div class="author"> Terrence Neumann, Sooyong Lee, Maria De-Arteaga, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sina Fazelpour, Matthew Lease' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2401.16558</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2401.16558" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The pervasive spread of misinformation and disinformation poses a significant threat to society. Professional fact-checkers play a key role in addressing this threat, but the vast scale of the problem forces them to prioritize their limited resources. This prioritization may consider a range of factors, such as varying risks of harm posed to specific groups of people. In this work, we investigate potential implications of using a large language model (LLM) to facilitate such prioritization. Because fact-checking impacts a wide range of diverse segments of society, it is important that diverse views are represented in the claim prioritization process. This paper examines whether a LLM can reflect the views of various groups when assessing the harms of misinformation, focusing on gender as a primary variable. We pose two central questions. (1) To what extent do prompts with explicit gender references reflect gender differences in opinion in the United States on topics of social relevance? and (2) To what extent do gender-neutral prompts align with gendered viewpoints on those topics? To analyze these questions, we present the TopicMisinfo dataset, containing 160 fact-checked claims from diverse topics, supplemented by nearly 1600 human annotations with subjective perceptions and annotator demographics. Analyzing responses to gender-specific and neutral prompts, we find that GPT 3.5-Turbo reflects empirically observed gender differences in opinion but amplifies the extent of these differences. These findings illuminate AI’s complex role in moderating online communication, with implications for fact-checkers, algorithm designers, and the use of crowd-workers as annotators. We also release the TopicMisinfo dataset to support continuing research in the community. </p> </div> </div> </div> </li> <li> <div class="row" data-trustworthy-ai-categories="safe,secure,privacy"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/prism_preview-480.webp 480w,/assets/img/publication_preview/prism_preview-800.webp 800w,/assets/img/publication_preview/prism_preview-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/prism_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="prism_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="neumann2024prism" class="col-sm-8"> <div class="title">PRISM: A Design Framework for Open-Source Foundation Model Safety</div> <div class="author"> Terrence Neumann, and Bryan Jones </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.10415</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.10415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The rapid advancement of open-source foundation models has brought transparency and accessibility to this groundbreaking technology. However, this openness has also enabled the development of highly-capable, unsafe models, as exemplified by recent instances such as WormGPT and FraudGPT, which are specifically designed to facilitate criminal activity. As the capabilities of open foundation models continue to grow, potentially outpacing those of closed-source models, the risk of misuse by bad actors poses an increasingly serious threat to society. This paper addresses the critical question of how open foundation model developers should approach model safety in light of these challenges. Our analysis reveals that open-source foundation model companies often provide less restrictive acceptable use policies (AUPs) compared to their closed-source counterparts, likely due to the inherent difficulties in enforcing such policies once the models are released. To tackle this issue, we introduce PRISM, a design framework for open-source foundation model safety that emphasizes Private, Robust, Independent Safety measures, at Minimal marginal cost of compute. The PRISM framework proposes the use of modular functions that moderate prompts and outputs independently of the core language model, offering a more adaptable and resilient approach to safety compared to the brittle reinforcement learning methods currently used for value alignment. By focusing on identifying AUP violations and engaging the developer community in establishing consensus around safety design decisions, PRISM aims to create a safer open-source ecosystem that maximizes the potential of these powerful technologies while minimizing the risks to individuals and society as a whole. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row" data-trustworthy-ai-categories="fair"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/does_ai_disp_preview-480.webp 480w,/assets/img/publication_preview/does_ai_disp_preview-800.webp 800w,/assets/img/publication_preview/does_ai_disp_preview-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/does_ai_disp_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="does_ai_disp_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="neumann2023does" class="col-sm-8"> <div class="title">Does AI-Assisted Fact-Checking Disproportionately Benefit Majority Groups Online?</div> <div class="author"> Terrence Neumann, and Nicholas Wolczynski </div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594013" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In recent years, algorithms have been incorporated into fact-checking pipelines. They are used not only to flag previously fact-checked misinformation, but also to provide suggestions about which trending claims should be prioritized for fact-checking - a paradigm called ’check-worthiness.’ While several studies have examined the accuracy of these algorithms, none have investigated how the benefits from these algorithms (via reduction in exposure to misinformation) are distributed amongst various online communities. In this paper, we investigate how diverse representation across multiple stages of the AI development pipeline affects the distribution of benefits from AI-assisted fact-checking for different online communities. We simulate information propagation through the network using our novel Topic-Aware, Community-Impacted Twitter (TACIT) simulator on a large Twitter followers network, tuned to produce realistic cascades of true and false information across multiple topics. Finally, using simulated data as a test bed, we implement numerous algorithmic fact-checking interventions that explicitly account for notions of diversity. We find that both representative and egalitarian methods for sampling and labeling check-worthiness model training data can lead to network-wide benefit concentrated in majority communities, while incorporating diversity into how fact-checkers use algorithmic recommendations can actively reduce inequalities in benefits between majority and minority communities. These findings contribute to an important conversation around the responsible implementation of AI-assisted fact-checking by social media platforms and fact-checking organizations.</p> </div> </div> </div> </li> <li> <div class="row" data-trustworthy-ai-categories="fair"> <div class="col col-sm-2 abbr"> </div> <div id="tanriverdi2023bias" class="col-sm-8"> <div class="title">Mitigating bias in organizational development and use of artificial intelligence</div> <div class="author"> </div> <div class="periodical"> <em>Proceedings of International Conference on Information Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>We theorize why some artificial intelligence (AI) algorithms unexpectedly treat protected classes unfairly. We hypothesize that mechanisms by which AI assumes agencies, rights, and responsibilities of its stakeholders can affect AI bias by increasing complexity and irreducible uncertainties: eg, AI’s learning method, anthropomorphism level, stakeholder utility optimization approach, and acquisition mode (make, buy, collaborate). In a sample of 726 agentic AI, we find that unsupervised and hybrid learning methods increase the likelihood of AI bias, whereas “strict” supervised learning reduces it. Highly anthropomorphic AI increases the likelihood of AI bias. Using AI to optimize one stakeholder’s utility increases AI bias risk, whereas jointly optimizing the utilities of multiple stakeholders reduces it. User organizations that co-create AI with developer organizations instead of developing it in-house or acquiring it off-the-shelf reduce AI bias risk. The proposed theory and the findings advance our understanding of responsible development and use of agentic AI.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row" data-trustworthy-ai-categories="fair"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/justice_in_misinfo_preview-480.webp 480w,/assets/img/publication_preview/justice_in_misinfo_preview-800.webp 800w,/assets/img/publication_preview/justice_in_misinfo_preview-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/justice_in_misinfo_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="justice_in_misinfo_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="neumann2022justice" class="col-sm-8"> <div class="title">Justice in misinformation detection systems: An analysis of algorithms, stakeholders, and potential harms</div> <div class="author"> Terrence Neumann, Maria De-Arteaga, and Sina Fazelpour </div> <div class="periodical"> <em>In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3531146.3533205" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.</p> </div> </div> </div> </li></ol> </div> <h2 id="background">Background</h2> <p>My path to academia included working as a Data Scientist at the <a href="https://crimelab.uchicago.edu/" rel="external nofollow noopener" target="_blank">University of Chicago Crime Lab</a>, where I was fortunate to collaborate with exceptional researchers <a href="https://harris.uchicago.edu/directory/jens-ludwig" rel="external nofollow noopener" target="_blank">Jens Ludwig</a> and <a href="https://www.maxkapustin.com/" rel="external nofollow noopener" target="_blank">Max Kapustin</a>. Prior to that, I received a MS in Analytics from Northwestern University and a BA in Economics and Mathematics from Indiana University, Bloomington. Outside of research, I love to run (I ran <a href="https://gobeyondracing.com/races/stumptown-trail-runs/" rel="external nofollow noopener" target="_blank">my first 50k in 2024</a>), cook, go to concerts, and play the guitar.</p> </div> <h2 class="news-heading"> <a href="/news/" style="color: inherit">News</a> </h2> <style>.news-heading{color:var(--global-text-color)!important}html[data-theme="dark"] .news-heading,html[data-theme="dark"] .news-heading a{color:#fffdd0!important}</style> <style>.news{color:var(--global-text-color)}html[data-theme="dark"] .news{color:#fffdd0!important}html[data-theme="dark"] .news table,html[data-theme="dark"] .news table th,html[data-theme="dark"] .news table td{color:#fffdd0!important}</style> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Nov 01, 2025</th> <td> My paper <a href="https://arxiv.org/abs/2504.08954" rel="external nofollow noopener" target="_blank">Should You Use LLMs to Simulate Opinions?</a> has been accepted to the <strong>AAAI Conference on Artificial Intelligence!</strong> I will update you when I hear about whether this will be a poster or presentation in the AI for Social Impact track. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> See you in Singapore this January! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 21, 2025</th> <td> My paper <a href="https://arxiv.org/abs/2504.08954" rel="external nofollow noopener" target="_blank">Should You Use LLMs To Simulate Opinions?</a> received the <strong>Best Paper, Runner Up Award</strong> for the ISS Cluster at INFORMS 2025. Thank you to the judges for taking the time to review my paper! </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 01, 2025</th> <td> I have been invited to present at INFORMS 2025 in the Generative AI session within the ISS Cluster! See you in Atlanta. </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%54%65%72%72%65%6E%63%65.%4E%65%75%6D%61%6E%6E@%6D%63%63%6F%6D%62%73.%75%74%65%78%61%73.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/terryneumann" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/terrence-neumann-6b711a6b" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=FXPuM7QAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Terrence Neumann. Last updated: November 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>